Data Classification is taking a training set of data and using it to create a model which can take a testing set and predict the class of the entries in the testing set. It is a type of supervised learning. Supervised learning is, as the name suggests, a form of machine learning where there is a supervising training dataset that is made by actual surveying or observation.

In general, for data clasification, the training set of data is available in the form of list of values(continuous or discrete) of one or more attributes and the class for each combination of attribute values. The testing dataset is obviously similar except that the class is to be predicted by the algorithm and the accuracy is checked against the real available data.

Here, we have a dataset in which the flowers have three species (classes) and four attributes (sepal length, sepal width, petal length, petal width).

There are various algorithms like logistic regression, decision trees, naive bayes, support vector machine, k-nearest neighbours, etc. for classification. Choosing the optimal classification algorithm is really important.

First of all, Naive Bayes is a pretty basic method, using completely basic mathematics of calculating posterior probability for various classifications and maximizing the probability. The formula used is P(c|x) = P(x|c)P(c)/P(x) which is the Naive Bayes Theorem. The classification with the maximum P(c|x) is chosen. But a great demerit of Naive Bayes Classifier is that it assumes the attributes to be conditionally independent.

Secondly, in logistic regression, we try to fit a regression line (or plane in case of 3 attributes or a point in case of 1 attribute) through the scatter plot to separate the classes such that all points on one side of the line are of one class and on the other side are of another class. This is done by modelling probabailities with sigmoid curve. For multiple classes, the same process is repeated for each pair classes. But the source page says that one class is linearly separable while other two are not, while logistic regression is a linear classifier.

Thirdly, k-nearest neighbours algorithm takes a test data and assigns it the class that is most common among its k nearest neighbours where k is specified by the user ('Nearest' in the sense of distance between the datapoints in the scatter plot). This algorithm obviously needs a huge amount of the data to make accurate predictions since it generalizes data on the basis of attribute similarity, but we have only 150 datapoints.

Finally, we are left with Decision trees and Support vector machines(SVM). Support vector machines have a huge theory behind their working, so I tried to skip it. The working of surface vector machines in a nutshell is based on bias/variance tradeoff wherein if the bias of a classifier is more, the variance is less and vice versa. SVM works on letting bias (misclassification) increase to reduce the variance (in classification of the test dataset). Using scikit-learn to classify (by first splitting the data in training and testing set), we see that svm gives a better accuracy than Decision trees. But I am sticking to decision trees since the mathematics for SVM seems complicated.

There are better algorithms that have evolved from decision trees like Bagging, Random forest, Boosting, Gradient Boosting, XGBoost, which show a better accuracy than decision but we skip them because of the difficult mathematics involved in them.

Trees are data structure that have a root node and nodes branching of from them recursively and ending in 'leaf' nodes. Binary trees have only at most 2 branching nodes from each node. Decision trees are usually binary trees that have a decision at each node and the class at the leaf nodes. If the decision is true we go to the left node and if it is false we go to the right node, recursively repeating the step until we reach a leaf node.

Here we have a dataset with 4 continuous attributes,and 3 classes. The simplest form of a dataset would be 1 discrete attribute and 2 classes. We start our explanation with that. Suppose we have a dataset of patient being obese as a YES or NO answer (discrete attribute) and the classes being having heart disease or not (2 classes YES and NO). If all the YES for obese correspond to all the YES of the heart disease and all the NO correspond to NO, we have a completely 'Pure' data. But real data is not like that, some YES may correspond to some NO and vice versa, sometimes the data could be missing (not in the Iris dataset, though). So we need some way to quantify the impurity, which will be useful later on for multible attributes. Now, obviously we see the majority of what attribute correponds to which class. If majority of YES for Heart disease have a YES for obese, then we make the decision as if obese, then has heart disease.

Now, to quantify the impurity we have two kinds of measures, Gini and Entropy. In the Gini method, let's say we take this example, we take p(y) as the probability of YES for heart disease if the person is obese and p(n) as probability of NO for heart disease if the person is obese. The Gini impurity is 1 - (p(y))^2 - ((p(n))^2. The Gini impurity is then taken for the other leaf (the one with the person not obese). The average of both is the total Gini impurity of the classification. 

Moving on to more number of attributes, we need to first decide which attribute makes the first decision. For that we see the Gini impurity of each attribute. The one with lower Gini impurity is taken. Again, for the next level of nodes we find the Gini impurities for all attributes other than the ones we already have taken into account but taking the subset of the data that goes under that node. We again take the smallest Gini impurity but this time we also see if it is less that the Gini impurity of the node itself. If it is we classify using that attribute, if it is not we make it a leaf node.

Now for more than one classes, we see one class vs all other classes for the two probabalities and we keep on doing that for each class until we find the class for which this gives us least Gini impurity.

If we have continuous data, we use need to split the attribute values into ranges. To find the best split we again use minimum Gini impurity scores for various splits. But there may be infinite splits so we try out some specific values like the midpoints of the adjacent values. Also, for continuous data, we need to consider all attributes (instead of the unvisited ones) in each node since the splitting pattern can change. 

Combining the various criteria for computing Gini impurities we get a lots of gini impurities for each nodes and we always take the split, the one vs all class and the attribute which gives the minimum Gini impurity.

The Entropy, on the other hand, is given by -p(y)log_2(p(y)) - p(n)log_2(p(n)). Every thing else about the approach remains the same. Here also we check for lower Entropy.

The Entropy and Gini have been defined for Binary trees only. If we want to use non-binary trees, we need not use the one vs all approach, rather we define Gini impurity as 1 - sum((p(i)^2) and Entropy as sum(-p(i)log_2(p_i)).
